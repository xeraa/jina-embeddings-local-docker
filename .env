# Quantization (used by docker-compose + download-model.sh)
#
# K = k-quant (block-wise quantization with per-block scaling, better quality
#     than older type-0 methods at the same bit width)
# M/S = medium/small tier within a k-quant level, controlling how many layers
#       get higher-precision treatment (M = balanced, S = more aggressive)
#
#   F16     ~424 MB   full 16-bit precision, best quality
#   Q8_0    ~215 MB   8-bit type-0 quant, near-lossless
#   Q6_K    ~170 MB   6-bit k-quant, near-lossless
#   Q5_K_M  ~150 MB   5-bit k-quant medium, great quality
#   Q4_K_M  ~130 MB   4-bit k-quant medium, good quality/size tradeoff
#   Q3_K_M  ~110 MB   3-bit k-quant medium, noticeable quality loss
#   Q2_K     ~90 MB   2-bit k-quant, significant quality loss
#
QUANT=F16

# Matryoshka embedding dimensions (used when creating ES pipeline + index)
#
# The server always outputs 768 dims. Matryoshka training makes the first N
# dimensions independently meaningful, so you can truncate for smaller/faster
# vectors with a graceful quality tradeoff.
#
#   768   ~3 KB/doc   full precision, maximum quality
#   512   ~2 KB/doc   minimal quality loss
#   256   ~1 KB/doc   good quality/size tradeoff
#   128  ~512 B/doc   compact, some quality loss
#    64  ~256 B/doc   coarse, for pre-filtering
#    32  ~128 B/doc   very coarse
#
EMBED_DIMS=768
